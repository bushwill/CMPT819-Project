{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1db7173",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9559f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import Crokinole as crk\n",
    "from config import DEFAULT_CONFIG\n",
    "from importlib import reload\n",
    "reload(crk)\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c7132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - loaded from config.py\n",
    "CONFIG = DEFAULT_CONFIG.copy()\n",
    "\n",
    "print(\"Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485e6673",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209b56e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load imageset.csv\n",
    "dataset_path = Path('images/imageset.csv')\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "print(f\"Loaded dataset with {len(df)} images\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48b1642",
   "metadata": {},
   "source": [
    "## Run Pipeline on All Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce58cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "results = []\n",
    "images_base_path = Path('images')\n",
    "\n",
    "# Process each image\n",
    "for idx, row in df.iterrows():\n",
    "    img_name = row['image_name']\n",
    "    img_path = images_base_path / img_name\n",
    "    \n",
    "    gt_team1 = int(row['team1_score'])\n",
    "    gt_team2 = int(row['team2_score'])\n",
    "    team1_20s = int(row['team1_20s'])\n",
    "    team2_20s = int(row['team2_20s'])\n",
    "    \n",
    "    print(f\"[{idx+1}/{len(df)}] {img_name}... \", end=\"\")\n",
    "    \n",
    "    # Run pipeline\n",
    "    pipeline_result = crk.run_complete_pipeline(\n",
    "        str(img_path),\n",
    "        CONFIG,\n",
    "        team1_20s=team1_20s,\n",
    "        team2_20s=team2_20s\n",
    "    )\n",
    "    \n",
    "    # Calculate relative percentage errors\n",
    "    if pipeline_result['success']:\n",
    "        pred_t1 = pipeline_result['team1_score']\n",
    "        pred_t2 = pipeline_result['team2_score']\n",
    "        \n",
    "        # Absolute errors\n",
    "        abs_error_team1 = abs(pred_t1 - gt_team1)\n",
    "        abs_error_team2 = abs(pred_t2 - gt_team2)\n",
    "        \n",
    "        # Percentage error calculation:\n",
    "        # If GT > 0: use relative error = |pred - gt| / gt * 100\n",
    "        # If GT = 0: just use absolute error (can't compute meaningful %)\n",
    "        error_pct_team1 = (abs_error_team1 / gt_team1 * 100) if gt_team1 > 0 else abs_error_team1\n",
    "        error_pct_team2 = (abs_error_team2 / gt_team2 * 100) if gt_team2 > 0 else abs_error_team2\n",
    "    else:\n",
    "        error_pct_team1 = None\n",
    "        error_pct_team2 = None\n",
    "        abs_error_team1 = None\n",
    "        abs_error_team2 = None\n",
    "    \n",
    "    # Store results\n",
    "    result_entry = {\n",
    "        'image_name': img_name,\n",
    "        'success': pipeline_result['success'],\n",
    "        'pred_team1': pipeline_result['team1_score'] if pipeline_result['success'] else None,\n",
    "        'pred_team2': pipeline_result['team2_score'] if pipeline_result['success'] else None,\n",
    "        'gt_team1': gt_team1,\n",
    "        'gt_team2': gt_team2,\n",
    "        'abs_error_team1': abs_error_team1,\n",
    "        'abs_error_team2': abs_error_team2,\n",
    "        'error_pct_team1': error_pct_team1,\n",
    "        'error_pct_team2': error_pct_team2,\n",
    "        'num_discs': len(pipeline_result['detected_discs']) if pipeline_result['detected_discs'] else 0,\n",
    "        'error_msg': pipeline_result['error']\n",
    "    }\n",
    "    results.append(result_entry)\n",
    "    \n",
    "    # Store overlay image for later display\n",
    "    result_entry['overlay_img'] = pipeline_result['overlay_img'] if pipeline_result['success'] else None\n",
    "    \n",
    "    # One-line result - format percentage or absolute based on GT\n",
    "    if pipeline_result['success']:\n",
    "        err_str_t1 = f\"{error_pct_team1:.1f}%\" if gt_team1 > 0 else f\"{abs_error_team1} pts\"\n",
    "        err_str_t2 = f\"{error_pct_team2:.1f}%\" if gt_team2 > 0 else f\"{abs_error_team2} pts\"\n",
    "        print(f\"OK Pred: T0={pred_t1}, T1={pred_t2} | GT: T0={gt_team1}, T1={gt_team2} | Error: T0={err_str_t1}, T1={err_str_t2}\")\n",
    "    else:\n",
    "        print(f\"FAILED: {pipeline_result['error']}\")\n",
    "\n",
    "print(f\"\\nProcessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ebcd28",
   "metadata": {},
   "source": [
    "## Display All Visualization Overlays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333358bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all overlay images with predictions and ground truth\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"VISUALIZATION OVERLAYS FOR ALL IMAGES\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for result in results:\n",
    "    if result['success'] and result['overlay_img'] is not None:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(result['overlay_img'])\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Calculate if prediction was correct\n",
    "        error_indicator = \"âœ“ PERFECT\" if result['total_error'] == 0 else f\"Error: {result['total_error']}\"\n",
    "        \n",
    "        title = f\"{result['image_name']}\\n\"\n",
    "        title += f\"Predicted: T0={result['pred_team1']}, T1={result['pred_team2']} | \"\n",
    "        title += f\"Ground Truth: T0={result['gt_team1']}, T1={result['gt_team2']}\\n\"\n",
    "        title += f\"{error_indicator}\"\n",
    "        \n",
    "        plt.title(title, fontsize=12, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    elif not result['success']:\n",
    "        print(f\"{result['image_name']}: FAILED - {result['error_msg']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0331264c",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0226659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe (excluding overlay images for CSV)\n",
    "results_for_df = [{k: v for k, v in r.items() if k != 'overlay_img'} for r in results]\n",
    "results_df = pd.DataFrame(results_for_df)\n",
    "\n",
    "# Display full results table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPLETE RESULTS TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Calculate statistics\n",
    "successful = results_df[results_df['success'] == True]\n",
    "num_success = len(successful)\n",
    "num_total = len(results_df)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total images processed: {num_total}\")\n",
    "print(f\"Successful detections: {num_success} ({100*num_success/num_total:.1f}%)\")\n",
    "print(f\"Failed detections: {num_total - num_success} ({100*(num_total-num_success)/num_total:.1f}%)\")\n",
    "\n",
    "if num_success > 0:\n",
    "    print(f\"\\nScoring Accuracy (successful detections only):\")\n",
    "    print(f\"  Mean percentage error (Team 0): {successful['error_pct_team1'].mean():.2f}%\")\n",
    "    print(f\"  Mean percentage error (Team 1): {successful['error_pct_team2'].mean():.2f}%\")\n",
    "    print(f\"  Mean absolute error (Team 0): {successful['abs_error_team1'].mean():.2f} points\")\n",
    "    print(f\"  Mean absolute error (Team 1): {successful['abs_error_team2'].mean():.2f} points\")\n",
    "    perfect = successful[(successful['abs_error_team1'] == 0) & (successful['abs_error_team2'] == 0)]\n",
    "    print(f\"  Perfect predictions (both teams exact): {len(perfect)} ({100*len(perfect)/num_success:.1f}%)\")\n",
    "    print(f\"  Mean discs detected: {successful['num_discs'].mean():.1f}\")\n",
    "\n",
    "# Save results to CSV\n",
    "output_path = Path('results/testset_results.csv')\n",
    "output_path.parent.mkdir(exist_ok=True)\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nResults saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f047966",
   "metadata": {},
   "source": [
    "## Visualize Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99223936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot per-image error distribution as dual bar graph\n",
    "if num_success > 0:\n",
    "    fig, ax = plt.subplots(figsize=(16, 6))\n",
    "    \n",
    "    # Get successful results sorted by image name\n",
    "    plot_data = successful.sort_values('image_name')\n",
    "    \n",
    "    x = np.arange(len(plot_data))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Create bars\n",
    "    bars1 = ax.bar(x - width/2, plot_data['error_pct_team1'], width, \n",
    "                   label='Team 0 Error %', color='skyblue', edgecolor='black')\n",
    "    bars2 = ax.bar(x + width/2, plot_data['error_pct_team2'], width, \n",
    "                   label='Team 1 Error %', color='lightcoral', edgecolor='black')\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_xlabel('Images', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Percentage Error (%)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Per-Image Error Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(plot_data['image_name'], rotation=45, ha='right', fontsize=8)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add mean lines\n",
    "    mean_t1 = plot_data['error_pct_team1'].mean()\n",
    "    mean_t2 = plot_data['error_pct_team2'].mean()\n",
    "    ax.axhline(mean_t1, color='blue', linestyle='--', linewidth=1, alpha=0.7, \n",
    "               label=f'Team 0 Mean: {mean_t1:.1f}%')\n",
    "    ax.axhline(mean_t2, color='red', linestyle='--', linewidth=1, alpha=0.7, \n",
    "               label=f'Team 1 Mean: {mean_t2:.1f}%')\n",
    "    ax.legend(fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No successful detections to visualize\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
